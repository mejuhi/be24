{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Unterminated string starting at: line 1 column 75059160 (char 75059159)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-614f4231cf6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[1;32min\u001b[0m \u001b[0myears\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmonth\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmonths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mmydict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0mfile_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'thisNewsNDAQ'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-614f4231cf6f>\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, year, month, key)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m                     return complexjson.loads(\n\u001b[1;32m--> 842\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m                     )\n\u001b[0;32m    844\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \"\"\"\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 1 column 75059160 (char 75059159)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import datetime\n",
    "test_start_date = datetime.datetime.now() - datetime.timedelta(days = 5)\n",
    "test_end_date = datetime.datetime.now() #- datetime.timedelta(days = 4)\n",
    "\n",
    "\n",
    "\n",
    "from newsapi import NewsAPI\n",
    "key = '96af62a035db45bda517a9ca62a25ac3'\n",
    "params = {}\n",
    "api = NewsAPI(key)\n",
    "sources = api.sources(params)\n",
    "articles = api.articles(sources[0]['id'], params)\n",
    "import sys, csv, json\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "import requests\n",
    "\"\"\"\n",
    "About:\n",
    "Python wrapper for the New York Times Archive API \n",
    "https://developer.nytimes.com/article_search_v2.json\n",
    "\"\"\"\n",
    "class APIKeyException(Exception):\n",
    "    def __init__(self, message): self.message = message \n",
    "class InvalidQueryException(Exception):\n",
    "    def __init__(self, message): self.message = message \n",
    "class ArchiveAPI(object):\n",
    "    def __init__(self, key=None):\n",
    "        \"\"\"\n",
    "        Initializes the ArchiveAPI class. Raises an exception if no API key is given.\n",
    "        :param key: New York Times API Key\n",
    "        \"\"\"\n",
    "        self.key = key\n",
    "        self.root = 'http://api.nytimes.com/svc/archive/v1/{}/{}.json?api-key={}' \n",
    "        if not self.key:\n",
    "            nyt_dev_page = 'http://developer.nytimes.com/docs/reference/keys'\n",
    "            exception_str = 'Warning: API Key required. Please visit {}'\n",
    "            raise NoAPIKeyException(exception_str.format(nyt_dev_page))\n",
    "\n",
    "    def query(self, year=None, month=None, key=None,):\n",
    "        \"\"\"\n",
    "        Calls the archive API and returns the results as a dictionary.\n",
    "        :param key: Defaults to the API key used to initialize the ArchiveAPI class.\n",
    "        \"\"\"\n",
    "        if not key: key = self.key\n",
    "        if (year < 1882) or not (0 < month < 13):\n",
    "            # currently the Archive API only supports year >= 1882\n",
    "            exception_str = 'Invalid query: See http://developer.nytimes.com/archive_api.json'\n",
    "            raise InvalidQueryException(exception_str)\n",
    "            \n",
    "        url = self.root.format(year, month, key)\n",
    "        r = requests.get(url)\n",
    "        return r.json()\n",
    "\n",
    "\n",
    "api = ArchiveAPI('0ba6dc04a8cb44e0a890c00df88c393a')\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "currentMonthStart=datetime.date.today().replace(day=1).strftime(\"%Y-%m-%d\")\n",
    "currentMonthStart\n",
    "years = [now.year]\n",
    "months = [now.month]\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        mydict = api.query(year, month)\n",
    "        file_str = 'thisNewsNDAQ' + '.json'\n",
    "        with open(file_str, 'w') as fout:\n",
    "            json.dump(mydict, fout)\n",
    "        fout.close()\n",
    "currentDateNow=datetime.datetime.now().strftime(\"%Y-%m-%d\")   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlefinance.client import get_price_data, get_prices_data, get_prices_time_data\n",
    "\n",
    "# Dow Jones\n",
    "param = {\n",
    "    'q': \".IXIC\", # Stock symbol (ex: \"AAPL\")\n",
    "    'i': \"86400\", # Interval size in seconds (\"86400\" = 1 day intervals)\n",
    "    'x': \"INDEXNASDAQ\", # Stock exchange symbol on which stock is traded (ex: \"NASD\")\n",
    "    'p': \"1M\" # Period (Ex: \"1Y\" = 1 year)\n",
    "}\n",
    "# get price data (return pandas dataframe)\n",
    "df = get_price_data(param)\n",
    "print(df)\n",
    "df.to_csv('thisPriceNDAQ.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv, json\n",
    "import pandas as pd\n",
    "\n",
    "################################################################################################\n",
    "## Preparing DJIA data\n",
    "# Reading DJIA index prices csv file\n",
    "with open('thisPriceNDAQ.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    # Converting the csv file reader to a lists \n",
    "    data_list = list(spamreader)\n",
    "\n",
    "# Separating header from the data\n",
    "header = data_list[0] \n",
    "data_list = data_list[1:] \n",
    "\n",
    "data_list = np.asarray(data_list)\n",
    "\n",
    "# Selecting date and close value for each day\n",
    "selected_data = data_list[:, [0, 4]]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=selected_data[0:,1:],\n",
    "             index=selected_data[0:,0],\n",
    "                                columns=['close'],\n",
    "                                        dtype='float64')\n",
    "\n",
    "\n",
    "\n",
    "df1 = df\n",
    "idx = pd.date_range(currentMonthStart, currentDateNow)\n",
    "df1.index = pd.DatetimeIndex(df1.index)\n",
    "df1 = df1.reindex(idx, fill_value=np.NaN)\n",
    "# df1.count() # gives 2518 count\n",
    "interpolated_df = df.interpolate()\n",
    "interpolated_df.count() # gives 3651 count\n",
    "\n",
    "# Removing extra date rows added in data for calculating interpolation\n",
    "interpolated_df = interpolated_df[3:]\n",
    "\n",
    "date_format = [\"%Y-%m-%dT%H:%M:%SZ\", \"%Y-%m-%dT%H:%M:%S+%f\"]\n",
    "def try_parsing_date(text):\n",
    "    for fmt in date_format:\n",
    "        #return datetime.strptime(text, fmt)\n",
    "        try:\n",
    "            return datetime.strptime(text, fmt).strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            pass\n",
    "    raise ValueError('no valid date format found')\n",
    "\n",
    "\n",
    "print ('### parsing date and price done##')\n",
    "\n",
    "\n",
    "\n",
    "years = [now.year]\n",
    "months = [now.month]\n",
    "dict_keys = ['pub_date', 'headline'] #, 'lead_paragraph']\n",
    "articles_dict = dict.fromkeys(dict_keys)\n",
    "# Filtering list for type_of_material\n",
    "type_of_material_list = ['blog', 'brief', 'news', 'editorial', 'op-ed', 'list','analysis']\n",
    "# Filtering list for section_name\n",
    "section_name_list = ['business', 'national', 'world', 'u.s.' , 'politics', 'opinion', 'tech', 'science',  'health']\n",
    "news_desk_list = ['business', 'national', 'world', 'u.s.' , 'politics', 'opinion', 'tech', 'science',  'health', 'foreign']\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "current_date = datetime.now().isoformat()\n",
    "\n",
    "#years = [2015]\n",
    "#months = [3]\n",
    "\n",
    "current_article_str = ''      \n",
    "\n",
    "## Adding article column to dataframe\n",
    "interpolated_df[\"articles\"] = ''\n",
    "count_articles_filtered = 0\n",
    "count_total_articles = 0\n",
    "count_main_not_exist = 0               \n",
    "count_unicode_error = 0     \n",
    "count_attribute_error = 0   \n",
    "for year in years:\n",
    "    for month in months:\n",
    "        file_str = 'thisNewsNDAQ' + '.json'\n",
    "        with open(file_str) as data_file:    \n",
    "            NYTimes_data = json.load(data_file)\n",
    "        count_total_articles = count_total_articles + len(NYTimes_data[\"response\"][\"docs\"][:])\n",
    "        \n",
    "        print ('Opening file:')\n",
    "        print (str(year) + '-' + '{:02}'.format(month))\n",
    "        \n",
    "        for i in range(len(NYTimes_data[\"response\"][\"docs\"][:])):\n",
    "            try:\n",
    "                if any(substring in NYTimes_data[\"response\"][\"docs\"][:][i]['type_of_material'].lower() for substring in type_of_material_list):\n",
    "                    if any(substring in NYTimes_data[\"response\"][\"docs\"][:][i]['section_name'].lower() for substring in section_name_list):\n",
    "                        #count += 1\n",
    "                        count_articles_filtered += 1\n",
    "                        #print 'i: ' + str(i)\n",
    "                        articles_dict = { your_key: NYTimes_data[\"response\"][\"docs\"][:][i][your_key] for your_key in dict_keys }\n",
    "                        articles_dict['headline'] = articles_dict['headline']['main'] # Selecting just 'main' from headline\n",
    "                        #articles_dict['headline'] = articles_dict['lead_paragraph'] # Selecting lead_paragraph\n",
    "                        date = try_parsing_date(articles_dict['pub_date'])\n",
    "                        #print 'article_dict: ' + articles_dict['headline']\n",
    "                        if date == current_date:\n",
    "                            current_article_str = current_article_str + '. ' + articles_dict['headline']\n",
    "                        else:  \n",
    "                            interpolated_df.set_value(current_date, 'articles', interpolated_df.loc[current_date, 'articles'] + '. ' + current_article_str)\n",
    "                            current_date = date\n",
    "                            #interpolated_df.set_value(date, 'articles', current_article_str)\n",
    "                            #print str(date) + current_article_str\n",
    "                            current_article_str = articles_dict['headline']\n",
    "                        # For last condition in a year\n",
    "                        if (date == current_date) and (i == len(NYTimes_data[\"response\"][\"docs\"][:]) - 1): \n",
    "                            interpolated_df.set_value(date, 'articles', current_article_str)   \n",
    "                        \n",
    "             #Exception for section_name or type_of_material absent\n",
    "            except AttributeError:\n",
    "                #print 'attribute error'\n",
    "                #print NYTimes_data[\"response\"][\"docs\"][:][i]\n",
    "                count_attribute_error += 1\n",
    "                # If article matches news_desk_list if none section_name found\n",
    "                try:\n",
    "                    if any(substring in NYTimes_data[\"response\"][\"docs\"][:][i]['news_desk'].lower() for substring in news_desk_list):\n",
    "                            #count += 1\n",
    "                            count_articles_filtered += 1\n",
    "                            #print 'i: ' + str(i)\n",
    "                            articles_dict = { your_key: NYTimes_data[\"response\"][\"docs\"][:][i][your_key] for your_key in dict_keys }\n",
    "                            articles_dict['headline'] = articles_dict['headline']['main'] # Selecting just 'main' from headline\n",
    "                            #articles_dict['headline'] = articles_dict['lead_paragraph'] # Selecting lead_paragraph\n",
    "                            date = try_parsing_date(articles_dict['pub_date'])\n",
    "                            #print 'article_dict: ' + articles_dict['headline']\n",
    "                            if date == current_date:\n",
    "                                current_article_str = current_article_str + '. ' + articles_dict['headline']\n",
    "                            else:  \n",
    "                                interpolated_df.set_value(current_date, 'articles', interpolated_df.loc[current_date, 'articles'] + '. ' + current_article_str)\n",
    "                                current_date = date\n",
    "                                #interpolated_df.set_value(date, 'articles', current_article_str)\n",
    "                                #print str(date) + current_article_str\n",
    "                                current_article_str = articles_dict['headline']\n",
    "                            # For last condition in a year\n",
    "                            if (date == current_date) and (i == len(NYTimes_data[\"response\"][\"docs\"][:]) - 1): \n",
    "                                interpolated_df.set_value(date, 'articles', current_article_str)   \n",
    "                \n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                pass\n",
    "            except KeyError:\n",
    "                print ('key error')\n",
    "                #print NYTimes_data[\"response\"][\"docs\"][:][i]\n",
    "                count_main_not_exist += 1\n",
    "                pass   \n",
    "            except TypeError:\n",
    "                print (\"type error\")\n",
    "                #print NYTimes_data[\"response\"][\"docs\"][:][i]\n",
    "                count_main_not_exist += 1\n",
    "                pass\n",
    "         \n",
    "\n",
    "              \n",
    "print (count_articles_filtered) \n",
    "print (count_total_articles)                     \n",
    "print (count_main_not_exist)\n",
    "print (count_unicode_error)\n",
    "\n",
    "\n",
    "\n",
    "## Putting all articles if no section_name or news_desk not found\n",
    "for date, row in interpolated_df.T.iteritems():   \n",
    "    if len(interpolated_df.loc[date, 'articles']) <= 400:\n",
    "        #print interpolated_df.loc[date, 'articles']\n",
    "        #print date\n",
    "        month = date.month\n",
    "        year = date.year\n",
    "        file_str = 'thisNews' + '.json'\n",
    "        with open(file_str) as data_file:    \n",
    "            NYTimes_data = json.load(data_file)\n",
    "        count_total_articles = count_total_articles + len(NYTimes_data[\"response\"][\"docs\"][:])\n",
    "        interpolated_df.set_value(date.strftime('%Y-%m-%d'), 'articles', '')\n",
    "        for i in range(len(NYTimes_data[\"response\"][\"docs\"][:])):\n",
    "            try:\n",
    "                \n",
    "                articles_dict = { your_key: NYTimes_data[\"response\"][\"docs\"][:][i][your_key] for your_key in dict_keys }\n",
    "                articles_dict['headline'] = articles_dict['headline']['main'] # Selecting just 'main' from headline\n",
    "                #articles_dict['headline'] = articles_dict['lead_paragraph'] # Selecting lead_paragraph       \n",
    "                pub_date = try_parsing_date(articles_dict['pub_date'])\n",
    "                #print 'article_dict: ' + articles_dict['headline']\n",
    "                if date.strftime('%Y-%m-%d') == pub_date: \n",
    "                    interpolated_df.set_value(pub_date, 'articles', interpolated_df.loc[pub_date, 'articles'] + '. ' + articles_dict['headline'])  \n",
    "                \n",
    "            except KeyError:\n",
    "                print ('key error')\n",
    "                #print NYTimes_data[\"response\"][\"docs\"][:][i]\n",
    "                #count_main_not_exist += 1\n",
    "                pass   \n",
    "            except TypeError:\n",
    "                print (\"type error\")\n",
    "                #print NYTimes_data[\"response\"][\"docs\"][:][i]\n",
    "                #count_main_not_exist += 1\n",
    "                pass\n",
    "\n",
    "\n",
    "#>>> print count_articles_filtered \n",
    "#440770\n",
    "#>>> print count_total_articles \n",
    "#1073132\n",
    "\n",
    "\n",
    "## Filtering the whole data for a year\n",
    "#filtered_data = interpolated_df.ix['2016-01-01':'2016-12-31']\n",
    "#filtered_data.to_pickle('/Users/Dinesh/Documents/Project Stock predictions/data/pickled_ten_year_all.pkl')  \n",
    "\n",
    "\n",
    "# Saving the data as pickle file\n",
    "interpolated_df.to_pickle('pickled_para_NDAQ.pkl')  \n",
    "\n",
    "\n",
    "# Save pandas frame in csv form\n",
    "interpolated_df.to_csv('sample_interpolated_para_NDAQ.csv',\n",
    "                       sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "# Reading the data as pickle file\n",
    "dataframe_read = pd.read_pickle('pickled_para_NDAQ.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the saved data pickle file\n",
    "df_stocks = pd.read_pickle('pickled_para_NDAQ.pkl')\n",
    "df_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stocks['prices'] = df_stocks['close'].apply(np.int64)\n",
    "# selecting the prices and articles\n",
    "df_stocks = df_stocks[['prices', 'articles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stocks['articles'] = df_stocks['articles'].map(lambda x: x.lstrip('.-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stocks\n",
    "df = df_stocks[['prices']].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding new columns to the data frame\n",
    "df[\"compound\"] = ''\n",
    "df[\"neg\"] = ''\n",
    "df[\"neu\"] = ''\n",
    "df[\"pos\"] = ''\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for date, row in df_stocks.T.iteritems():\n",
    "    try:\n",
    "        sentence = unicodedata.normalize('NFKD', df_stocks.loc[date, 'articles'])\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        df.set_value(date, 'compound', ss['compound'])\n",
    "        df.set_value(date, 'neg', ss['neg'])\n",
    "        df.set_value(date, 'neu', ss['neu'])\n",
    "        df.set_value(date, 'pos', ss['pos'])\n",
    "    except TypeError:\n",
    "        print (df_stocks.loc[date, 'articles'])\n",
    "        print (date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import time\n",
    "#import datetime\n",
    "\n",
    "\n",
    "test = df.ix[test_start_date:test_end_date]\n",
    "\n",
    "\n",
    "##\n",
    "#test_start_date = '2018-03-09'\n",
    "#test_end_date = '2018-03-10'\n",
    "test = df.ix[test_start_date:test_end_date]\n",
    "  \n",
    "##    \n",
    "    \n",
    "# Calculating the sentiment score\n",
    "sentiment_score_list = []\n",
    "for date, row in test.T.iteritems():\n",
    "    sentiment_score = np.asarray([df.loc[date, 'compound'],df.loc[date, 'neg'],df.loc[date, 'neu'],df.loc[date, 'pos']])\n",
    "    #sentiment_score = np.asarray([df.loc[date, 'neg'],df.loc[date, 'pos']])\n",
    "    sentiment_score_list.append(sentiment_score)\n",
    "    numpy_df_test = np.asarray(sentiment_score_list)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'finalized_model_NDAQ.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#loaded_model.fit(numpy_df_test,test['prices'])   \n",
    "result = loaded_model.predict(numpy_df_test)\n",
    "\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ret = loaded_model.score(numpy_df_test,test['prices'])\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
